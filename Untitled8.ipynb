{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandarosma/TF-IDF-and-Cosine-Similarity/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf9zqBN1s1WN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROVTM7UMqHOU",
        "outputId": "9dfeefb0-ea8f-497e-ab5b-37ed8654dd9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyTelegramBotAPI\n",
            "  Downloading pytelegrambotapi-4.26.0-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pyTelegramBotAPI) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pyTelegramBotAPI) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pyTelegramBotAPI) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pyTelegramBotAPI) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pyTelegramBotAPI) (2024.12.14)\n",
            "Downloading pytelegrambotapi-4.26.0-py3-none-any.whl (270 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m266.2/270.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.5/270.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyTelegramBotAPI\n",
            "Successfully installed pyTelegramBotAPI-4.26.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyTelegramBotAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "FNKc7E8st7k6",
        "outputId": "4de0d860-d43b-4ac0-b089-dc7d49cdeed3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'laptop_datas.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-334e45c0469b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"laptop_datas.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Combine relevant columns to create a 'Specifications' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'laptop_datas.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords and punkt tokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"laptop_datas.csv\", sep=';')\n",
        "\n",
        "# Combine relevant columns to create a 'Specifications' column\n",
        "data['Specifications'] = data[['ScreenResolution', 'Cpu', 'Ram', 'Memory', 'Gpu', 'OpSys']].apply(lambda x: ', '.join(x.astype(str)), axis=1)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    tokens = [word for word in text.lower().split() if word.isalnum()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['Processed_Specifications'] = data['Specifications'].apply(preprocess_text)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Processed_Specifications'])\n",
        "\n",
        "# Test function to calculate similarity\n",
        "def recommend_laptop(query):\n",
        "    # Preprocess user query\n",
        "    query_processed = preprocess_text(query)\n",
        "    query_vector = tfidf_vectorizer.transform([query_processed])\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Find top 3 similar laptops\n",
        "    top_indices = similarities.argsort()[-3:][::-1]\n",
        "    recommendations = data.iloc[top_indices][['Company', 'TypeName', 'Specifications']]\n",
        "\n",
        "    if similarities[top_indices[0]] == 0:\n",
        "        return \"Maaf, tidak ada laptop yang sesuai dengan pencarian Anda.\"\n",
        "    return recommendations.to_string(index=False)\n",
        "\n",
        "# Example query\n",
        "user_query = \"windows 10\"\n",
        "print(recommend_laptop(user_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rDaqcEyr745",
        "outputId": "1959a50d-ab8a-468e-fde9-25884e0f8d79"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'telebot'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b6a0061927f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtelebot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'telebot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import telebot\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Inisialisasi bot Telegram\n",
        "BOT_TOKEN = '7705017743:AAFoHQpzQGH9XsHHP07VxXLjeDvvCENy-yQ'\n",
        "bot = telebot.TeleBot(BOT_TOKEN)\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('laptop_datas.csv', sep=';')  # Ganti dengan nama file CSV Anda\n",
        "\n",
        "# Gabungkan semua spesifikasi menjadi satu string untuk setiap laptop\n",
        "def combine_specs(row):\n",
        "    # Sesuaikan nama kolom dengan dataset Anda\n",
        "    specs = f\"{row['Company']} {row['TypeName']} {row['Inches']} {row['ScreenResolution']} {row['Cpu']} {row['Ram']} {row['Memory']} {row['Gpu']} {row['OpSys']} {row['Weight']} {row['Price']}\"\n",
        "    return specs.lower()\n",
        "\n",
        "def keyword_filter(user_query):\n",
        "    \"\"\"\n",
        "    Filter dataset berdasarkan kata kunci khusus yang ditemukan di query pengguna.\n",
        "    \"\"\"\n",
        "    query = preprocess_text(user_query)\n",
        "    keywords = query.split()\n",
        "\n",
        "    # Filter awal dataset\n",
        "    filtered_df = df.copy()\n",
        "\n",
        "    # Cek kata kunci tertentu\n",
        "    if \"ringan\" in keywords or \"tipis\" in keywords:\n",
        "        filtered_df = filtered_df[filtered_df['Weight'].str.replace('kg', '').str.strip().astype(float) < 2]\n",
        "\n",
        "    if \"editing\" in keywords:\n",
        "        # Contoh: Filter laptop dengan RAM >= 8GB dan GPU\n",
        "        filtered_df = filtered_df[\n",
        "            (filtered_df['RAM'].str.contains(\"8GB|16GB|32GB\", case=False, na=False)) &\n",
        "            (filtered_df['Gpu'].str.contains(\"Nvidia|AMD|Intel\", case=False, na=False))\n",
        "        ]\n",
        "\n",
        "    if \"murah\" in keywords or \"terjangkau\" in keywords:\n",
        "        # Contoh: Laptop dengan harga <= 10 juta\n",
        "        filtered_df = filtered_df[filtered_df['Price'].astype(float) <= 10000000]\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "# Buat kolom baru berisi gabungan spesifikasi\n",
        "df['combined_specs'] = df.apply(combine_specs, axis=1)\n",
        "\n",
        "\n",
        "# Preprocessing text\n",
        "def preprocess_text(text):\n",
        "    # Mengubah ke lowercase\n",
        "    text = text.lower()\n",
        "    # Menghapus karakter khusus\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "    # Menghapus stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Menggabungkan kembali\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess semua spesifikasi\n",
        "df['processed_specs'] = df['combined_specs'].apply(preprocess_text)\n",
        "\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(df['processed_specs'])\n",
        "\n",
        "def get_laptop_recommendations(user_query, top_n=3, threshold=0.1):\n",
        "    # Preprocess query pengguna\n",
        "    processed_query = preprocess_text(user_query)\n",
        "\n",
        "    # Transform query menggunakan TF-IDF\n",
        "    query_vector = tfidf.transform([processed_query])\n",
        "\n",
        "    # Hitung cosine similarity\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
        "\n",
        "    # Ambil top N rekomendasi yang melewati threshold\n",
        "    top_indices = np.argsort(similarities)[::-1]\n",
        "    recommendations = []\n",
        "\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] >= threshold:\n",
        "            laptop = df.iloc[idx]\n",
        "            # Sesuaikan format output dengan kolom yang ada di dataset Anda\n",
        "            rec_text = (\n",
        "                f\"Merk: {laptop['Company']}\\n\"\n",
        "                f\"Model: {laptop['TypeName']}\\n\"\n",
        "                f\"Display: {laptop['Inches']}\\n\"\n",
        "                f\"Resolution: {laptop['ScreenResolution']}\\n\"\n",
        "                f\"Storage: {laptop['Cpu']}\\n\"\n",
        "                f\"RAM: {laptop['Ram']}\\n\"\n",
        "                f\"Memory: {laptop['Memory']}\\n\"\n",
        "                f\"Processor: {laptop['Gpu']}\\n\"\n",
        "                f\"OS: {laptop['OpSys']}\\n\"\n",
        "                f\"Weight: {laptop['Weight']}\\n\"\n",
        "                f\"Price: {laptop['Price']}\\n\"\n",
        "                f\"Similarity Score: {similarities[idx]:.2f}\\n\"\n",
        "                f\"-------------------\"\n",
        "            )\n",
        "            recommendations.append(rec_text)\n",
        "\n",
        "            if len(recommendations) >= top_n:\n",
        "                break\n",
        "\n",
        "    if not recommendations:\n",
        "        return [\"Maaf, tidak dapat menemukan laptop yang sesuai dengan kriteria Anda. Mohon coba dengan kata kunci lain.\"]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "\n",
        "# Handler untuk pesan selamat datang\n",
        "@bot.message_handler(commands=['start'])\n",
        "def send_welcome(message):\n",
        "    welcome_text = (\n",
        "        \"Selamat datang di Bot Rekomendasi Laptop! 👋\\n\\n\"\n",
        "        \"Saya akan membantu Anda menemukan laptop yang sesuai dengan kebutuhan Anda.\\n\\n\"\n",
        "        \"Contoh cara bertanya:\\n\"\n",
        "        \"- Laptop untuk gaming dengan RTX 3060\\n\"\n",
        "        \"- Laptop RAM 16GB untuk editing\\n\"\n",
        "        \"- Laptop tipis ringan untuk kuliah\\n\"\n",
        "        \"- Laptop intel core i7 dengan SSD\\n\\n\"\n",
        "        \"Silakan ajukan pertanyaan Anda!\"\n",
        "    )\n",
        "    bot.reply_to(message, welcome_text)\n",
        "\n",
        "# Handler untuk pesan teks\n",
        "@bot.message_handler(func=lambda message: True)\n",
        "def handle_message(message):\n",
        "    try:\n",
        "        # Dapatkan rekomendasi\n",
        "        recommendations = get_laptop_recommendations(message.text)\n",
        "\n",
        "        # Kirim setiap rekomendasi\n",
        "        bot.reply_to(message, \"🔍 Berikut rekomendasi laptop untuk Anda:\")\n",
        "        for rec in recommendations:\n",
        "            bot.send_message(message.chat.id, rec)\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.reply_to(message, \"Maaf, terjadi kesalahan. Silakan coba lagi.\")\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "#  Daftar respons\n",
        "responses = [\n",
        "    \"Ini respons pertama.\",\n",
        "    \"Bagaimana kalau ini?\",\n",
        "    \"Mungkin ini menarik?\",\n",
        "    \"Ini pilihan lain untukmu.\",\n",
        "    \"Coba lihat yang satu ini.\"]\n",
        "\n",
        "# Handler untuk pesan alternatif\n",
        "@bot.message_handler(func=lambda message: True)\n",
        "def handle_message(message):\n",
        "    text = message.text.lower()\n",
        "    if \"kalau yang lain\" in text or \"selain ini\" in text:\n",
        "        # Mengambil respons acak dari daftar\n",
        "        response = random.choice(responses)\n",
        "        bot.reply_to(message, response)\n",
        "    else:\n",
        "        bot.reply_to(message, \"Saya belum mengerti maksudmu. Coba katakan 'kalau yang lain?' atau 'selain ini'.\")\n",
        "\n",
        "\n",
        "# Jalankan bot\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Bot sedang berjalan...\")\n",
        "    bot.polling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOerSahwBjY5"
      },
      "outputs": [],
      "source": [
        "import telebot\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Inisialisasi bot Telegram\n",
        "BOT_TOKEN = '7705017743:AAFoHQpzQGH9XsHHP07VxXLjeDvvCENy-yQ'\n",
        "bot = telebot.TeleBot(BOT_TOKEN)\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('laptop_datas.csv', sep=';')  # Sesuaikan nama file CSV Anda\n",
        "\n",
        "# Gabungkan semua spesifikasi menjadi satu string untuk setiap laptop\n",
        "def combine_specs(row):\n",
        "    specs = f\"{row['Company']} {row['TypeName']} {row['Inches']} {row['ScreenResolution']} {row['Cpu']} {row['Ram']} {row['Memory']} {row['Gpu']} {row['OpSys']} {row['Weight']} {row['Price']}\"\n",
        "    return specs.lower()\n",
        "\n",
        "df['combined_specs'] = df.apply(combine_specs, axis=1)\n",
        "\n",
        "# Preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['processed_specs'] = df['combined_specs'].apply(preprocess_text)\n",
        "\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(df['processed_specs'])\n",
        "\n",
        "# Daftar respons alternatif\n",
        "alt_responses = [\n",
        "    \"Bagaimana kalau yang ini?\",\n",
        "    \"Coba lihat pilihan lainnya.\",\n",
        "    \"Ini mungkin menarik bagi Anda.\",\n",
        "    \"Ada rekomendasi lain yang bisa dipertimbangkan.\",\n",
        "]\n",
        "\n",
        "# Fungsi mendapatkan rekomendasi\n",
        "def get_laptop_recommendations(user_query, top_n=3, threshold=0.1):\n",
        "    processed_query = preprocess_text(user_query)\n",
        "    query_vector = tfidf.transform([processed_query])\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
        "\n",
        "    top_indices = np.argsort(similarities)[::-1]\n",
        "    recommendations = []\n",
        "\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] >= threshold:\n",
        "            laptop = df.iloc[idx]\n",
        "            rec_text = (\n",
        "                f\"Merk: {laptop['Company']}\\n\"\n",
        "                f\"Model: {laptop['TypeName']}\\n\"\n",
        "                f\"Display: {laptop['Inches']}\\n\"\n",
        "                f\"Resolution: {laptop['ScreenResolution']}\\n\"\n",
        "                f\"RAM: {laptop['Ram']}\\n\"\n",
        "                f\"Storage: {laptop['Memory']}\\n\"\n",
        "                f\"Processor: {laptop['Gpu']}\\n\"\n",
        "                f\"OS: {laptop['OpSys']}\\n\"\n",
        "                f\"Weight: {laptop['Weight']}\\n\"\n",
        "                f\"Price: {laptop['Price']}\\n\"\n",
        "                f\"Similarity Score: {similarities[idx]:.2f}\\n\"\n",
        "                f\"-------------------\"\n",
        "            )\n",
        "            recommendations.append(rec_text)\n",
        "\n",
        "            if len(recommendations) >= top_n:\n",
        "                break\n",
        "\n",
        "    if not recommendations:\n",
        "        return [\"Maaf, tidak ada laptop yang cocok dengan kriteria Anda.\"]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Handler untuk perintah /start\n",
        "@bot.message_handler(commands=['start'])\n",
        "def send_welcome(message):\n",
        "    # Bubble pertama\n",
        "    welcome_text_1 = (\n",
        "        \"Selamat datang di Bot Rekomendasi Laptop! 👋\\n\\n\"\n",
        "    )\n",
        "    bot.reply_to(message, welcome_text_1)\n",
        "\n",
        "    # Bubble kedua\n",
        "    welcome_text_2 = (\n",
        "        \"Dengan Lapbo, saya akan membantu Anda menemukan laptop yang sesuai dengan kebutuhan Anda.\\n\\n\"\n",
        "        \"Selalu sertakan kata kunci 'Laptop' di setiap pertanyaan agar memudahkan pencarian Anda.\\n\\n\"\n",
        "        \"Silakan ajukan pertanyaan Anda!\"\n",
        "    )\n",
        "    bot.send_message(message.chat.id, welcome_text_2)\n",
        "\n",
        "\n",
        "# Handler untuk pesan teks\n",
        "@bot.message_handler(func=lambda message: True)\n",
        "def handle_message(message):\n",
        "    try:\n",
        "        text = message.text.lower()\n",
        "\n",
        "        # Respons alternatif\n",
        "        if \"kalau yang lain\" in text or \"selain ini\" in text:\n",
        "            response = random.choice(alt_responses)\n",
        "            bot.reply_to(message, response)\n",
        "            return\n",
        "\n",
        "        # Rekomendasi laptop\n",
        "        recommendations = get_laptop_recommendations(message.text)\n",
        "        bot.reply_to(message, \"🔍 Berikut rekomendasi laptop untuk Anda:\")\n",
        "        for rec in recommendations:\n",
        "            bot.send_message(message.chat.id, rec)\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.reply_to(message, \"Maaf, terjadi kesalahan. Silakan coba lagi.\")\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Jalankan bot\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Bot sedang berjalan...\")\n",
        "    bot.polling()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFhXrP8DsVxh"
      },
      "source": [
        "COBA DISINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "hv5-pQBjdd8d",
        "outputId": "8e36e84d-b1c3-4a67-8ae4-b3b03a3491f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-aa82596ed06e>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_specs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combined_specs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Inisialisasi TF-IDF Vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-aa82596ed06e>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import telebot\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Inisialisasi bot Telegram\n",
        "BOT_TOKEN = '7705017743:AAFoHQpzQGH9XsHHP07VxXLjeDvvCENy-yQ'\n",
        "bot = telebot.TeleBot(BOT_TOKEN)\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('laptop_datas.csv', sep=';')  # Sesuaikan nama file CSV Anda\n",
        "\n",
        "# Gabungkan semua spesifikasi menjadi satu string untuk setiap laptop\n",
        "def combine_specs(row):\n",
        "    specs = f\"{row['Company']} {row['TypeName']} {row['Inches']} {row['ScreenResolution']} {row['Cpu']} {row['Ram']} {row['Memory']} {row['Gpu']} {row['OpSys']} {row['Weight']} {row['Price']}\"\n",
        "    return specs.lower()\n",
        "\n",
        "df['combined_specs'] = df.apply(combine_specs, axis=1)\n",
        "\n",
        "# Preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['processed_specs'] = df['combined_specs'].apply(preprocess_text)\n",
        "\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(df['processed_specs'])\n",
        "\n",
        "# Daftar respons alternatif\n",
        "alt_responses = [\n",
        "    \"Bagaimana kalau yang ini?\",\n",
        "    \"Coba lihat pilihan lainnya.\",\n",
        "    \"Ini mungkin menarik bagi Anda.\",\n",
        "    \"Ada rekomendasi lain yang bisa dipertimbangkan.\",\n",
        "]\n",
        "\n",
        "# Fungsi mendapatkan rekomendasi\n",
        "def get_laptop_recommendations(user_query, top_n=4, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Fungsi untuk mendapatkan rekomendasi laptop berdasarkan query pengguna.\n",
        "    Hanya menampilkan jumlah maksimal `top_n` hasil.\n",
        "    \"\"\"\n",
        "    processed_query = preprocess_text(user_query)\n",
        "    query_vector = tfidf.transform([processed_query])\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
        "\n",
        "    top_indices = np.argsort(similarities)[::-1]\n",
        "    recommendations = []\n",
        "\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] >= threshold:\n",
        "            laptop = df.iloc[idx]\n",
        "            rec_text = (\n",
        "                f\"Merk: {laptop['Company']}\\n\"\n",
        "                f\"Model: {laptop['TypeName']}\\n\"\n",
        "                f\"Display: {laptop['Inches']} inch\\n\"\n",
        "                f\"Resolution: {laptop['ScreenResolution']}\\n\"\n",
        "                f\"RAM: {laptop['Ram']}\\n\"\n",
        "                f\"Storage: {laptop['Memory']}\\n\"\n",
        "                f\"Processor: {laptop['Cpu']}\\n\"\n",
        "                f\"GPU: {laptop['Gpu']}\\n\"\n",
        "                f\"OS: {laptop['OpSys']}\\n\"\n",
        "                f\"Weight: {laptop['Weight']}\\n\"\n",
        "                f\"Price: {laptop['Price']}\\n\"\n",
        "                f\"-------------------\"\n",
        "            )\n",
        "            recommendations.append(rec_text)\n",
        "\n",
        "            if len(recommendations) >= top_n:\n",
        "                break\n",
        "\n",
        "    if not recommendations:\n",
        "        return [\"Maaf, tidak ada laptop yang cocok dengan kriteria Anda.\"]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Handler untuk perintah /start\n",
        "@bot.message_handler(commands=['start'])\n",
        "def send_welcome(message):\n",
        "    welcome_text = (\n",
        "        \"Selamat datang di Bot Rekomendasi Laptop! 👋\\n\\n\"\n",
        "        \"Saya akan membantu Anda menemukan laptop yang sesuai dengan kebutuhan Anda.\\n\\n\"\n",
        "        \"Silakan ajukan pertanyaan Anda! Jangan lupa sertai kata kunci 'laptop' disetiap pertanyaan agar lebih memudahkan anda. Terimakasih.\"\n",
        "    )\n",
        "    bot.reply_to(message, welcome_text)\n",
        "\n",
        "# Handler untuk pesan teks\n",
        "@bot.message_handler(func=lambda message: True)\n",
        "def handle_message(message):\n",
        "    try:\n",
        "        text = message.text.lower()\n",
        "\n",
        "        # Respons alternatif\n",
        "        if \"kalau yang lain\" in text or \"selain ini\" in text:\n",
        "            response = random.choice(alt_responses)\n",
        "            bot.reply_to(message, response)\n",
        "            return\n",
        "\n",
        "        # Rekomendasi laptop\n",
        "        recommendations = get_laptop_recommendations(message.text)\n",
        "        bot.reply_to(message, \"🔍 Berikut rekomendasi laptop untuk Anda:\")\n",
        "        for rec in recommendations:\n",
        "            bot.send_message(message.chat.id, rec)\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.reply_to(message, \"Maaf, terjadi kesalahan. Silakan coba lagi.\")\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Jalankan bot\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Bot sedang berjalan...\")\n",
        "    bot.polling()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "vNTIPHPasUjA",
        "outputId": "21ba136d-12fc-4eea-95a8-71ea1dbcae1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot sedang berjalan...\n"
          ]
        }
      ],
      "source": [
        "import telebot\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Inisialisasi bot Telegram\n",
        "BOT_TOKEN = '7705017743:AAFoHQpzQGH9XsHHP07VxXLjeDvvCENy-yQ'\n",
        "bot = telebot.TeleBot(BOT_TOKEN)\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('laptop_datas.csv', sep=';')  # Sesuaikan nama file CSV Anda\n",
        "\n",
        "# Gabungkan semua spesifikasi menjadi satu string untuk setiap laptop\n",
        "def combine_specs(row):\n",
        "    specs = f\"{row['Company']} {row['TypeName']} {row['Inches']} {row['ScreenResolution']} {row['Cpu']} {row['Ram']} {row['Memory']} {row['Gpu']} {row['OpSys']} {row['Weight']} {row['Price']}\"\n",
        "    return specs.lower()\n",
        "\n",
        "df['combined_specs'] = df.apply(combine_specs, axis=1)\n",
        "\n",
        "# Preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['processed_specs'] = df['combined_specs'].apply(preprocess_text)\n",
        "\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(df['processed_specs'])\n",
        "\n",
        "# Daftar respons alternatif\n",
        "alt_responses = [\n",
        "    \"Bagaimana kalau yang ini?\",\n",
        "    \"Coba lihat pilihan lainnya.\",\n",
        "    \"Ini mungkin menarik bagi Anda.\",\n",
        "    \"Ada rekomendasi lain yang bisa dipertimbangkan.\",\n",
        "]\n",
        "\n",
        "# Fungsi mendapatkan rekomendasi\n",
        "def get_laptop_recommendations(user_query, top_n=4, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Fungsi untuk mendapatkan rekomendasi laptop berdasarkan query pengguna.\n",
        "    Hanya menampilkan jumlah maksimal `top_n` hasil.\n",
        "    \"\"\"\n",
        "    processed_query = preprocess_text(user_query)\n",
        "    query_vector = tfidf.transform([processed_query])\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
        "\n",
        "    top_indices = np.argsort(similarities)[::-1]\n",
        "    recommendations = []\n",
        "\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] >= threshold:\n",
        "            laptop = df.iloc[idx]\n",
        "            rec_text = (\n",
        "                f\"Merk: {laptop['Company']}\\n\"\n",
        "                f\"Model: {laptop['TypeName']}\\n\"\n",
        "                f\"Display: {laptop['Inches']} inch\\n\"\n",
        "                f\"Resolution: {laptop['ScreenResolution']}\\n\"\n",
        "                f\"RAM: {laptop['Ram']}\\n\"\n",
        "                f\"Storage: {laptop['Memory']}\\n\"\n",
        "                f\"Processor: {laptop['Cpu']}\\n\"\n",
        "                f\"GPU: {laptop['Gpu']}\\n\"\n",
        "                f\"OS: {laptop['OpSys']}\\n\"\n",
        "                f\"Weight: {laptop['Weight']}\\n\"\n",
        "                f\"Price: {laptop['Price']}\\n\"\n",
        "                f\"-------------------\"\n",
        "            )\n",
        "            recommendations.append(rec_text)\n",
        "\n",
        "            if len(recommendations) >= top_n:\n",
        "                break\n",
        "\n",
        "    if not recommendations:\n",
        "        return [\"Maaf, tidak ada laptop yang cocok dengan kriteria Anda.\"]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Handler untuk perintah /start\n",
        "@bot.message_handler(commands=['start'])\n",
        "def send_welcome(message):\n",
        "    welcome_text = (\n",
        "        \"Selamat datang di Bot Rekomendasi Laptop! 👋\\n\\n\"\n",
        "        \"Saya akan membantu Anda menemukan laptop yang sesuai dengan kebutuhan Anda.\\n\\n\"\n",
        "        \"Saran saya gunakan kata kunci 'laptop' di setiap pertanyaan anda!\\n\\n\"\n",
        "        \"Silakan ajukan pertanyaan Anda!\"\n",
        "    )\n",
        "    bot.reply_to(message, welcome_text)\n",
        "\n",
        "# Handler untuk pesan teks\n",
        "@bot.message_handler(func=lambda message: True)\n",
        "def handle_message(message):\n",
        "    try:\n",
        "        text = message.text.lower()\n",
        "\n",
        "        # Respons alternatif\n",
        "        if \"kalau yang lain\" in text or \"selain ini\" in text:\n",
        "            response = random.choice(alt_responses)\n",
        "            bot.reply_to(message, response)\n",
        "            return\n",
        "\n",
        "        # Rekomendasi laptop\n",
        "        recommendations = get_laptop_recommendations(message.text)\n",
        "        bot.reply_to(message, \"🔍 Berikut rekomendasi laptop untuk Anda:\")\n",
        "        for rec in recommendations:\n",
        "            bot.send_message(message.chat.id, rec)\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.reply_to(message, \"Maaf, terjadi kesalahan. Silakan coba lagi.\")\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Jalankan bot\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Bot sedang berjalan...\")\n",
        "    bot.polling()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}